## If false, Controlcenter (C3) will not be installed
##
enabled: false 
## C3 License information
##
license: ""
## Name of C3 cluster
## 
name: controlcenter
image:
  repository: confluentinc/cp-enterprise-control-center-operator
  tag: 5.4.1.0
## Pod resources
##
resources:
  ## It is recommended to set both resource requests and limits.
  ## If not configured, kubernetes will set cpu/memory defaults.
  ## Reference: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  requests:
    cpu: 500m
    memory: 4Gi
  limits: {}
## JVM configuration  
##
jvmConfig:
  heapSize: 4G
## Volume size
##
volume:
  data0: 10Gi
## Pod termination grace-period
##
terminationGracePeriodSeconds: 60
## C3 TLS configuration
##
tls:
  enabled: false
  port: 443
  authentication:
    type: ""
  fullchain: |-
  privkey: |-
  cacerts: |-

## External Access
##
loadBalancer:
  ## Create a LoadBalancer for external networking
  enabled: false
  ## External will create public facing endpoints, setting this to internal will
  ## create a private-facing ELB with VPC peering
  type: external
  ## Add other annotations here that you want on the ELB
  annotations: {}
  ## If external access is enable, the fqdn must be provided to access c3
  ##
  domain: ""
  ## If prefix is configured, external DNS name is configured as <prefix>.<domain>
  ## otherwise external DNS name is configured as <name>.<domain> where name is cluster-name.
  prefix: ""

## All dependencies components for Control-Center
##
dependencies:
  ## Kafka cluster C3 uses internally
  ##
  c3KafkaCluster:
    tls: 
      ## If true, TLS configuration is enable
      ##
      enabled: false
      ## Supported authentication types: plain, tls
      ##
      authentication:
         type: ""
      ## If true, inter-communication will be encrypted if TLS is enabled. The bootstrapEndpoint will have FQDN name.
      ## If false, the security setting is configured to use either SASL_PLAINTEXT or PLAINTEXT
      internal: false
    ## If above tls.internal is true, configure with Kafka bootstrap DNS configuration on port 9092 e.g <kafka.name>.<domain>:9092
    ## If above tls.internal is false, configure with Kafka service name on port 9071 eg. <kafka.name>:9071 or FQDN name of Kafka serivce name e.g <name>.<namespace>.svc.cluster.local:9071
    bootstrapEndpoint: ""
    ## Broker initial count configuration
    ##
    brokerCount: 3
    ## Zookeeper service name with port 2181 eg zookeeper:2181
    ##
    zookeeper:
      endpoint: ""
  ## C3 monitoring clusters
  ##
  monitoringKafkaClusters: []
  # monitoringKafkaClusters:
  #  - name: kafka-test
  #    tls:
  #      enabled: true
  #      internal: false
  #      authentication:
  #        type: plain
  #    bootstrapEndpoint: "kafka-prod1:9071"
  #    ## Use if destination Kafka cluster have a different username/password
  #    ## then global username/password configured for SASL security protocol.
  #    username: test-demo
  #    password: test-demo-password
  #  - name: kafka-test1
  #    tls:
  #      enabled: true
  #      internal: false
  #    bootstrapEndpoint: "kafka-prod2:9071"

  ## KSQL configurations
  ##  
  ksql:
    enabled: false
    # this one should be reachable from the machine where C3 is installed (use internal ksql service name with port 9088, ef http://ksql:9088)
    url: ""
    # this one should be reachable from the machine where the browser using C3 is running
    # e.g http|s://schemaregistry.example.com:|9081|8081
    advertisedUrl: ""
    tls:
      enabled: false
      authentication:
        type: ""
  ##
  ## SchemaRegistry Configurations  
  schemaRegistry:
    enabled: false
    ## e.g http|s://schemaregistry:9081|8081
    ##
    url: ""
    tls:
      enabled: false
      authentication:
        type: ""
  ## Connect cluster configurations
  ##
  connectCluster:
    enabled: false
    ## connect cluster endpoint
    ## http|s://<connector-svc-name>:9081|8081
    url: ""
    tls:
      enabled: false
      authentication:
        type: ""
    # ZooKeeper connection string for the Kafka cluster backing the Connect cluster.
    # If unspecified, falls back to the the c3KafkaCluster's zookeeper configurations
    zookeeperEndpoint: ""
    # Bootstrap servers for Kafka cluster backing the Connect cluster
    # If unspecified, falls back to the the c3KafkaCluster's bootstrapEndpoint configurations
    kafkaBootstrapEndpoint: ""
    timeout: 15000


## C3 authentication
##
auth:
  basic:
    enabled: false
    ##
    ## map with key as user and value as password and role
    property: {}
    # property:
    #  admin: Developer1,Administrators
    #  disallowed: no_access
  ldap:
    enabled: false
    nameOfGroupSearch: c3users
    property: {}
    # property:
    #  useLdaps : "true"
    #  contextFactory: "com.sun.jndi.ldap.LdapCtxFactory"
    #  hostname: ""
    #  port: "389"
    #  bindDn: ""
    #  bindPassword: ""
    #  authenticationMethod: ""
    #  forceBindingLogin: "true"
    #  userBaseDn: ""
    #  userRdnAttribute: "sAMAccountName"
    #  userIdAttribute: "sAMAccountName"
    #  userPasswordAttribute: "userPassword"
    #  userObjectClass: "user"
    #  roleBaseDn: ""
    #  roleNameAttribute: "cn"
    #  roleMemberAttribute: "member"
    #  roleObjectClass: "group"

mail:
  enabled: false
  hostname: ""
  port: 587
  from: ""
  bounceAddress: ""
  sslCheckserveridentity: false
  starttlsRequired: false
  username: ""
  password: ""

alert:
  email: "alert@example.com"

debug: false

## Pod distribution on nodes with given key and values
## The node Affinity configuration uses preferredDuringSchedulingIgnoredDuringExecution
## https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity#
nodeAffinity : {}
#nodeAffinity:
#  key: components
#  values:
#  - controlcenter
#  - app

## Pod Anti-Affinity
## It uses preferredDuringSchedulingIgnoredDuringExecution
## https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#interlude-built-in-node-labels
## Use this capability, if the cluster has somekind of concept of racks
rack: {}
#rack:
# topology: kubernetes.io/hostname

## Disable HostPort
## This is mechanism to isolate pods of same type running on the same node through port mapping on the host.
## If this feature is true, make sure to use nodeAffinity and rack to distribute pod across nodes.
## Take precaution before enabling it
disableHostPort: true
##
## Inject pod-level annotations
## Any changes on this field will trigger rolling restart
##
alphaPodAnnotations: {}
#alphaPodAnnotations:
#  string: "value"
#  number: "1"
#  boolean: "true"
#  list: "[{\"labels\": {\"key\": \"value\"}},{\"key1\": \"value1\"}]"
